---
layout: post
title: "PCA, a new kid is in town"
description: ""
headline: "the missing pieces"
categories: ML
tags: [sparsity,PCA]
imagefeature: draw-on-board.jpg
comments: true
mathjax: true
featured: false
published: true
---

"Sparse?"


When most elements of a relative matrix are zero, we may say this matrix sparse. We often prefer the matries to be sparse, because they can be stored with less memory and computed faster with dedicate data structure and algorithms. Essentially, sparse encoding is a technique which convert a dense (data) matrix to sparse matrix. This may sound just like principal component analysis, PCA. However, the two have totally different approaches. 


Consider a 100-question survey, when 500 samples' data collected it can be represented as a 100 by 500 matrix. Experience tells us, often time the questions are interrelated. Sometime, several questions are even asking the same things but in different ways. Intuitively, we know this is not a 100-dimension problem. Let's say it is an x-dimension problem. We cannot just pick x most important questions out of the 100, because questions are relative, or in statistic jargon, dependent. PCA is a technique to helps to find the x dimensions.


