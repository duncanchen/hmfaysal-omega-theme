<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
<title type="text">SPARSITY</title>
<generator uri="https://github.com/mojombo/jekyll">Jekyll</generator>
<link rel="self" type="application/atom+xml" href="/feed.xml" />
<link rel="alternate" type="text/html" href="" />
<updated>2015-04-30T03:28:10-04:00</updated>
<id>/</id>
<author>
  <name>Duncan Chen</name>
  <uri>/</uri>
  <email>duncanchen(at)gmail.com</email>
</author>


<entry>
  <title type="html"><![CDATA[PCA, More or Less]]></title>
  <link>/ml/PCA-more-or-less</link>
  <id>/ml/PCA-more-or-less</id>
  <published>2015-04-15T00:00:00-04:00</published>
  <updated>2015-04-15T00:00:00-04:00</updated>
  <author>
    <name>Duncan Chen</name>
    <uri></uri>
    <email>duncanchen(at)gmail.com</email>
  </author>
  <content type="html">&lt;p&gt;How far would you guess that the Principal Component Analysis (PCA) can be traced back? It &lt;a href=&quot;http://imedea.uib-csic.es/master/cambioglobal/Modulo_V_cod101615/Theory/lit_support/pca_wold.pdf&quot; target=&quot;_blacnk&quot;&gt;was developed&lt;/a&gt; in the early 20th century…1901 to be exact. PCA started gaining the popularity in 1930s, when it was introduced in phonology research. Since then, it has been widely adopted in social science research. But don’t think PCA is a less rigid methodology because of its popularity in the social sciences. In fact, PCA is derived from solid mathematical methods. Can we explain PCA with as little jargon as possible?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://bit.ly/1O6zLn6&quot; alt=&quot;dimesion-projection&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;ananlogy&quot;&gt;Ananlogy&lt;/h2&gt;
&lt;p&gt;Let’s start with an analogy first. Say we would like to study a group of fish inside a tank by recording their positions relative to one another. However, there is one problem: we are restricted to using a projector and its resulting projections to collect data. We can place the projector at any angle we’d like. Dimension-wise, this condition restricts us to using two-dimensional data to study a three-dimensional world, even though each two-dimension projection is still a snapshot of the original three-dimensional reality. Under such a restriction, the projector’s position is big a deal, because a poorly-chosen position would not be able to reveal the relative positions among the fish.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://bit.ly/1JId4Pz&quot; alt=&quot;position&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We now turn to a real-world scenario similar to the analogy. Say we conduct a survey, which consists of 100 questions—a 100-dimensional fish tank. We can perform analytics as 100-variable models, but we rarely do so in practice, for a couple of reasons. First, a questionnaire is often designed to have multiple questions for one topic. The answers of these questions are, therefore, highly correlated. Not only would it be redundant, it would also lead to a large estimated error if highly correlated variables exist (a topic for another day). Second, we may believe there are some hidden structures among the variables. Each of the hidden variables is still made of the original variables; however, we may be able to explain the data better through the hidden variables, or structure. Therefore, there are two questions:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Where can we place our “projector” over the data, so as to preserve the greatest separation among the data?&lt;/li&gt;
  &lt;li&gt;How many dimensions does the final projection have?&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;pca&quot;&gt;PCA&lt;/h2&gt;

&lt;p&gt;This is where PCA comes is. PCA belongs to a category of statistical method, called multiple variable analysis. In particular, it is a method of dimension reduction. It solves the problems we proposed the following way:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;First to the projector position problem, we need to place the projector, right?  In fact, we don’t. We fix the projector’s position and rotate the data, i.e., the fish tank, instead.  This has the same effect. The rotation of data is merely a coordinate transform from one (basis of the vector space, in linear algebra’s terms) to another. Notice in this stage, we have not reduced the dimension yet.&lt;/li&gt;
  &lt;li&gt;To effectively reduce the dimension, the new coordinates have to be orthogonal. Orthogonal coordinates allow us to sort the axes by the difference each axis reveals or accounts for. As you may have guessed, the number of axes we select is the number of dimensions of the projection. This is why PCA is called “Principle,” because it only takes the major coordinates revealing a sufficient difference among data. The “Component” in PCA is just another name for coordinate.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;So to sum up PCA, it is a dimension reduction method. It greatly simplifies the complexity of a large-dimension problem. The principal components may give us insight of the underlying variable structures, but it is not a guarantee. Because, while we have reduced the problem to fewer dimensions, each new dimension– i.e. a principal component– is still a linear combination of all the original variables. When original variables appear evenly in the new basis, they are still hard to interpret. To avoid this trouble, Sparse PCA was developed. We will look into the idea, Sparsity, in the next post.&lt;/p&gt;


  &lt;p&gt;&lt;a href=&quot;/ml/PCA-more-or-less&quot;&gt;PCA, More or Less&lt;/a&gt; was originally published by Duncan Chen at &lt;a href=&quot;&quot;&gt;SPARSITY&lt;/a&gt; on April 15, 2015.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[about sparsity]]></title>
  <link>/site/about-sparsity</link>
  <id>/site/about-sparsity</id>
  <published>2015-04-13T00:00:00-04:00</published>
  <updated>2015-04-13T00:00:00-04:00</updated>
  <author>
    <name>Duncan Chen</name>
    <uri></uri>
    <email>duncanchen(at)gmail.com</email>
  </author>
  <content type="html">&lt;p&gt;“Principal component analysis is outdated by sparse coding.”&lt;/p&gt;

&lt;p&gt;“Why?”&lt;/p&gt;

&lt;p&gt;“Uh…, let me think it through and I will write it down.”&lt;/p&gt;


  &lt;p&gt;&lt;a href=&quot;/site/about-sparsity&quot;&gt;about sparsity&lt;/a&gt; was originally published by Duncan Chen at &lt;a href=&quot;&quot;&gt;SPARSITY&lt;/a&gt; on April 13, 2015.&lt;/p&gt;</content>
</entry>

</feed>
