<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
<title type="text">SPARSITY</title>
<generator uri="https://github.com/mojombo/jekyll">Jekyll</generator>
<link rel="self" type="application/atom+xml" href="/feed.xml" />
<link rel="alternate" type="text/html" href="" />
<updated>2015-04-19T04:48:59-04:00</updated>
<id>/</id>
<author>
  <name>Duncan Chen</name>
  <uri>/</uri>
  <email>duncanchen(at)gmail.com</email>
</author>


<entry>
  <title type="html"><![CDATA[PCA, a new kid is in town]]></title>
  <link>/ml/sparsity</link>
  <id>/ml/sparsity</id>
  <published>2015-04-15T00:00:00-04:00</published>
  <updated>2015-04-15T00:00:00-04:00</updated>
  <author>
    <name>Duncan Chen</name>
    <uri></uri>
    <email>duncanchen(at)gmail.com</email>
  </author>
  <content type="html">&lt;p&gt;How far will you guess the Principal Component Analysis, PCA, can be traced back? PCA &lt;a href=&quot;http://imedea.uib-csic.es/master/cambioglobal/Modulo_V_cod101615/Theory/lit_support/pca_wold.pdf&quot; target=&quot;_blacnk&quot;&gt;was developed&lt;/a&gt; in early 20 century, 1901 to be exactly. It started gaining the popularity in 1930s. Since then, it has been widely applied in the social science researches. But don’t think PCA a less rigid methodology because of its application in social science. In fact, PCA is derived from solid mathematical methods. Can we explain PCA with as less jargons as possible?   I’d like to give it a try.&lt;/p&gt;

&lt;p&gt;PCA belongs to a category of statistical method, called multiple variable analysis. By multiple, it can be a lot. Say we conduct an online survey of 100 questions and that’s 100 variables or dimensions.  Experience tells us that often time the questions were interrelated. Sometime, several questions were even asking the same things but in different ways. Intuitively, we know this is not a 100-dimension problem. But how do we get it down to less dimensions?  That’s where PCA comes in. It provides a way to reduce the original problem to less dimensions. I would say the first thing we need to remember of PCA is it is dimension reduction method. It implies some information may get ignored after the dimension reduced. These bring two challenges to us:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Preserve the information as much as possible.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Find a ranking method, so we can select the important few.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The key to these challenge is &lt;em&gt;transportaion&lt;/em&gt;.  &lt;/p&gt;

&lt;p&gt;To a that, we can combine the original variables to create a new set of variables, exactly the same number as the original. This new set of variables, &lt;/p&gt;

&lt;p&gt;As to determine which dimensions to stay, we of course like keep as much information as possible – the information can explain the &lt;em&gt;difference&lt;/em&gt; among the data.&lt;br /&gt;
&amp;gt;FTo manifest the  the The selected dimension formed a subspace, in mathematical term.&lt;/p&gt;


  &lt;p&gt;&lt;a href=&quot;/ml/sparsity&quot;&gt;PCA, a new kid is in town&lt;/a&gt; was originally published by Duncan Chen at &lt;a href=&quot;&quot;&gt;SPARSITY&lt;/a&gt; on April 15, 2015.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[about sparsity]]></title>
  <link>/site/about-sparsity</link>
  <id>/site/about-sparsity</id>
  <published>2015-04-13T00:00:00-04:00</published>
  <updated>2015-04-13T00:00:00-04:00</updated>
  <author>
    <name>Duncan Chen</name>
    <uri></uri>
    <email>duncanchen(at)gmail.com</email>
  </author>
  <content type="html">&lt;p&gt;“Principal component analysis is outdated by sparse coding.”&lt;/p&gt;

&lt;p&gt;“Why?”&lt;/p&gt;

&lt;p&gt;“Uh…, let me think it through and I will write it down.”&lt;/p&gt;


  &lt;p&gt;&lt;a href=&quot;/site/about-sparsity&quot;&gt;about sparsity&lt;/a&gt; was originally published by Duncan Chen at &lt;a href=&quot;&quot;&gt;SPARSITY&lt;/a&gt; on April 13, 2015.&lt;/p&gt;</content>
</entry>

</feed>
